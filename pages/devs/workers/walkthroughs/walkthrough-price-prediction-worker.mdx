import { Callout } from 'nextra/components'

# Walkthrough: Build and Deploy Price Prediction Worker Node

> How to build a node that predicts the future price of Ether

## Prerequisites 

1. Make sure you have checked the documentation on how to [build and deploy a worker node from scratch](./build-and-deploy-worker-from-scratch).
2. Clone the [basic-coin-prediction-node](https://github.com/allora-network/basic-coin-prediction-node) repository. It will serve as the base sample for your quick setup.

We will work from the repository you just cloned. We will explain each part of the source code and make changes to your custom setup as required. Additionally, we encourage you to check the repository's [README](https://github.com/allora-network/basic-coin-prediction-node/blob/main/README.md) for further guidance.

## Setting up your Custom Prediction Node network

### Generate Keys

As explained previously, any worker node is a peer in the Allora Network and must be uniquely identified. Generate keys for your nodes by running the commands below (this will automatically create the necessary keys and IDs in the necessary folders): 

**Create head keys:**

```
docker run -it --entrypoint=bash -v ./head-data:/data alloranetwork/allora-inference-base:latest -c "mkdir -p /data/keys && (cd /data/keys && allora-keys)"
```

**Create worker keys**

```
docker run -it --entrypoint=bash -v ./worker-data:/data alloranetwork/allora-inference-base:latest -c "mkdir -p /data/keys && (cd /data/keys && allora-keys)"
```

<Callout type="info">
**Important note:** If no keys are specified in the volumes, new keys will be automatically created inside the`head-data/keys` and `worker-data/keys` when first generating keys.
</Callout>

### Connect the worker node to the head node

After both worker and head nodes' identities are generated inside the `head-data/keys` and `worker-data/keys` directories, instruct the worker node to connect to the head node:

- run `cat head-data/keys/identity` to extract the head node's peer_id specified in the `head-data/keys/identity`
- use the printed peer_id to replace the `head-id` placeholder value specified inside the `docker-compose.yml` file when running the worker service: `--boot-nodes=/ip4/172.22.0.100/tcp/9010/p2p/<head-id>`

### Run setup  
- Once all the above is set up, run `docker-compose build && docker-compose up`  
  - This will bring up the head, the worker, and the inference nodes (which will run an initial update using the `updater`). 
    - The `updater` node is a companion for updating the inference node state and is meant to hit the `/update` endpoint on the inference service. It is expected to run periodically, being crucial for maintaining the accuracy of the inferences.  
  - You can verify that your services are up by running `docker-compose ps` and run the test curl request [below](/devs/workers/walkthroughs/walkthrough-price-prediction-worker#issue-an-execution-request) with the appropriate arguments.

### Understanding Docker Compose Services

Look into the `docker-compose.yml` to change specific values as needed. The docker-compose file consists of 4 docker services, namely:

1. **Inference (`app.py`)**:
   - Exposes endpoints to generate inference and update the model.
   - Interacts with the model to generate inference or update the model.
   - Serves as the gateway to the model from external requests.
   - Customizable logic to suit your use case.

2. **Updater (`update_app.py`)**:
   - Hits the `/update` endpoint on the inference service.
   - Ensures the model state is updated when needed.
   - Should be scheduled for periodic updates based on use case.

3. **Worker**:
   - Combines `allora-inference-base`, the node function, and custom `main.py`.
   - Receives requests from the head node and broadcasts them to workers.
   - Downloads the function from IPFS and runs it to call `main.py`.
   - Uses `/inference/<token>` endpoint to channel requests to the model server.
   - Built on `Dockerfile_b7s`, extending `allora-inference-base`.
   - `BOOT_NODES` variable specifies the _head node address_ and _peer_id_ for communication.

4. **Head**:
   - Represents an Allora network head node for local testing.
   - Emulates the network but is not required for running the node in the Allora network.

## Issue an Execution Request

After the node is running locally, it may be queried. Using cURL, issue the following HTTP request to the head node

```json json
curl --location 'http://localhost:6000/api/v1/functions/execute' \
--header 'Content-Type: application/json' \
--data '{
    "function_id": "bafybeigpiwl3o73zvvl6dxdqu7zqcub5mhg65jiky2xqb4rdhfmikswzqm",
    "method": "allora-inference-function.wasm",
    "parameters": null,
    "topic": "1",
    "config": {
        "env_vars": [
            {
                "name": "BLS_REQUEST_PATH",
                "value": "/api"
            },
            {
                "name": "ALLORA_ARG_PARAMS",
                "value": "ETH"
            }
        ],
        "number_of_nodes": -1,
        "timeout": 2
    }
}'
```

The result should look like this:

```json json
{
  "code": "200",
  "request_id": "03001a39-4387-467c-aba1-c0e1d0d44f59",
  "results": [
    {
      "result": {
        "stdout": "{\"value\":\"2564.021586281073\"}",
        "stderr": "",
        "exit_code": 0
      },
      "peers": [
        "12D3KooWG8dHctRt6ctakJfG5masTnLaKM6xkudoR5BxLDRSrgVt"
      ],
      "frequency": 100
    }
  ],
  "cluster": {
    "peers": [
      "12D3KooWG8dHctRt6ctakJfG5masTnLaKM6xkudoR5BxLDRSrgVt"
    ]
  }
}
```

Where the `results.result.stdout` provides the prediction output in json format.

## Deploying your Custom Prediction Node

To get your node deployed to a remote production environment, you can deploy however you prefer or follow our Kubernetes [deployment guide](./build-and-deploy-worker-from-scratch#deploying-a-worker-node) where you:

1. Add the universal-helm chart to the helm repo.
2. Update the `values.yaml` file to suit your case.
3. Install the universal helm chart, and it will automatically deploy the node to production with the provided values.
4. Monitor the node in your Kubernetes cluster.
