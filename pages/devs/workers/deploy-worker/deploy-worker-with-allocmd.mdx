# Building a Worker Node with the allocmd CLI


The `allocmd` CLI tool simplifies the creation and deployment of worker nodes. It bootstraps all necessary components, eliminating the need to write worker nodes from scratch. 

To build a worker node with allocmd, follow these steps:

### 1. Install `allocmd` CLI

First, [install `allocmd`](/devs/get-started/installation/cli#installing-allocmd).

### 2. Generate worker scaffold files

Next, initialize the CLI to bootstrap all the needed components to get your worker running. The following command will handle the initialization process. It will create all the files in the appropriate directories and generate identities for your node to be used for local development.

```shell
allocmd generate worker --name <workerName> --topic <topicId> --env dev
cd <workerName>/worker
```

> Note: if you're facing `Permission denied` issues, please try to create the folders before running the `allocmd generate` command:
> ```shell
> mkdir -p <workerName>/worker/data/head
> mkdir -p <workerName>/worker/data/worker
> chmod -R 777 ./<workerName>/worker/data/head
> chmod -R 777 ./<workerName>/worker/data/worker
> ```

Before running this command you will have to [pick the topic Id](/devs/topic-creators/existing-topics) you wish to generate inference for after which you can run this command with the topic Id. The command will auto-create some files, the most important of which is the `dev-docker-compose.yaml`file which is an already complete docker-compose that you can run immediately to see your worker and head nodes running on your local machine. You can edit the files as you wish. For instance, the `main.py` is meant for you to call your inference server, hence you will have to edit the sample code with actual URLs and logic as you prefer.

After you have written and tested your logic in `main.py`, you can then run

```shell
docker-compose -f dev-docker-compose.yaml up --build
```

You should then be see the logs from the nodes, and be able to make a request to your head node and see it get a response from the worker node. Note that in production, you won't be the one to make the inference request, as the Allora chain will do this at the `EpochLength` provided by the topic creator.

You can test your node by running the following curl command:

```
curl --location 'http://localhost:6000/api/v1/functions/execute' --header 'Accept: application/json, text/plain, */*' --header 'Content-Type: application/json;charset=UTF-8' --data '{
    "function_id": "bafybeigpiwl3o73zvvl6dxdqu7zqcub5mhg65jiky2xqb4rdhfmikswzqm",
    "method": "allora-inference-function.wasm",
    "parameters": null,
    "topic": "<TOPIC_ID>",
    "config": {
        "env_vars": [
            {                              
                "name": "BLS_REQUEST_PATH",
                "value": "/api"
            },
            {                              
                "name": "ALLORA_ARG_PARAMS",
                "value": "<argument>"
            }
        ],
        "number_of_nodes": -1,
        "timeout" : 2
    }
}' | jq
```

The `<TOPIC_ID>` needs to be [an existing topic on the chain](/devs/topic-creators/existing-topics). The `<argument>` is what the topic is expecting to receive to perform the inference (as an indication to test, you can use the `DefaultArg`  value from the topic on-chain, e.g. for ETH prediction topic, it should be `"ETH"`).

### 3. Initialize the worker for production

Your worker node is now ready to be deployed, the `main.py` has been modified, all env variables passed, and the worker node is running locally and you are now ready to deploy your worker to run in the production environment. The following command will handle the generation of the `prod-docker-compose.yaml` file which contains all the keys and parameters needed for your worker to function perfectly in production.
> Note: make sure you run this command in the `<workerName>/worker` folder

```shell
allocmd generate worker --env prod
chmod -R +rx ./data/scripts
```

By running this command, `prod-docker-compose.yaml` will be generated with appropriate keys and parameters. You can now run the docker-compose file or deploy the whole codebase in your preferred cloud instance. At this stage, your worker should be responding to inference request from the Allora Chain!
