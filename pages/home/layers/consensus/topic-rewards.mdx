import { Callout } from 'nextra/components'

# Calculating Topic Reward Distribution Across all Network Participants

Now that we've explained the mechanisms behind distributing rewards for the actors of each network participant, let's dive in to how topic rewards are distributed across the groups of::
- inference workers
- forecast workers 
- reputers

The common objective of the calculated reward distribution across the network is to incentivize decentralization.

## Key Factors
### Modified Entropy

Entropy, in this context, is a measure of how spread out the rewards are among participants. We calculate entropy for each class of tasks (inference, forecast, reputer), which helps in determining how decentralized the reward distribution is.

<Callout type="info">
Higher entropy means rewards are more evenly spread out across all participants.
</Callout>

The modified entropy for each class of tasks is given by the following equations:

**Inference**

$$
F_i = - \sum_j f_{ij} \ln \left( f_{ij} \left( \frac{N_i^{\text{eff}}}{N_i} \right)^\beta \right)
$$

Where:
- $$ F_i $$ is the entropy for inference workers.
- $$ \sum_j $$ means we add up the values for all inference workers $$ j $$.
- $$ f_{ij} $$ is the (smoothed) fraction of rewards for the $$ j $$-th inference worker.
- $$ N_i^{\text{eff}} $$ is the effective number of inference workers (a fair count to prevent cheating).
- $$ N_i $$ is the total number of inference workers.
- $$ \beta $$ is a constant that helps adjust the calculation.

The formula for forecast workers ($$ G_i $$) and reputers ($$ H_i $$) is similar:

$$
G_i = - \sum_k f_{ik} \ln \left( f_{ik} \left( \frac{N_f^{\text{eff}}}{N_f} \right)^\beta \right)
$$

$$
H_i = - \sum_m f_{im} \ln \left( f_{im} \left( \frac{N_r^{\text{eff}}}{N_r} \right)^\beta \right)
$$

Where:
- $$ G_i $$ and $$ H_i $$ are the entropies for forecast workers and reputers, respectively.
- $$ \sum_k $$ and $$ \sum_m $$ mean we add up the values for all forecast workers $$ k $$ and all reputers $$ m $$.
- $$ f_{ik} $$ and $$ f_{im} $$ are the (smoothed) fractions of rewards for the $$ k $$-th forecast worker and the $$ m $$-th reputer.
- $$ N_f^{\text{eff}} $$ and $$ N_r^{\text{eff}} $$ are the effective numbers of forecast workers and reputers.
- $$ N_f $$ and $$ N_r $$ are the total numbers of forecast workers and reputers.


where we have defined modified reward fractions per class as:

$$
f_{ij} = \frac{\tilde{u}_{ij}}{\sum_j \tilde{u}_{ij}}, \quad f_{ik} = \frac{\tilde{v}_{ik}}{\sum_k \tilde{v}_{ik}}, \quad f_{im} = \frac{\tilde{w}_{im}}{\sum_m \tilde{w}_{im}}
$$

<Callout type="info">
    Here, the tilde over the rewards indicates they are smoothed using an exponential moving average to remove noise and volatility from the decentralization measure.
</Callout>

### Effective Number of Participants

To prevent manipulation of the reward system against sybil attacks, we calculate the effective number of participants (actors). It ensures that the reward distribution remains fair even if someone tries to game the system.

$$
N_i^{\text{eff}} = \frac{1}{\sum_j f_{ij}^2}, \quad N_f^{\text{eff}} = \frac{1}{\sum_k f_{ik}^2}, \quad N_r^{\text{eff}} = \frac{1}{\sum_m f_{im}^2}
$$

Where:
- $$ N_i^{\text{eff}} $$, $$ N_f^{\text{eff}} $$, and $$ N_r^{\text{eff}} $$ are the effective numbers of inference workers, forecast workers, and reputers.
- The fractions $$ f_{ij} $$, $$ f_{ik} $$, and $$ f_{im} $$ are squared and then added up for each type of worker.


## Putting It All Together

### Dividing the Pie: Who Gets What?

We take the total reward for a task and split it among the different worker types based on our entropy calculations. Here's the formula:

$$
U_i = \frac{(1 - \chi)\gamma F_i E_{i,t}}{F_i + G_i + H_i}, \quad V_i = \frac{\chi \gamma G_i E_{i,t}}{F_i + G_i + H_i}, \quad W_i = \frac{H_i E_{i,t}}{F_i + G_i + H_i}
$$

In simpler terms:
- $$ U_i $$ is the reward for inference workers.
- $$ V_i $$ is the reward for forecast workers.
- $$ W_i $$ is the reward for reputers.
- $$ E_{i,t} $$ is the total reward for the participants in topic $$ t $$.
- $$ \chi $$ is a factor that adjusts how much reward goes to forecast workers.
- $$ \gamma $$ is a is a normalization factor to ensure the rewards add up to $$ E_{i,t} $$.
- $$ F_i $$, $$ G_i $$, and $$ H_i $$ are the entropies for inference workers, forecast workers, and reputers.

### What Value is Added by Forecasters? Checking the Predictions

We quantify the value added by the entire forecasting task using a score called $$ T_i $$:

$$
T_i = \log L_i^- - \log L_i
$$

Where:
- $$ T_i $$ is the performance score for the entire forecasting task.
- $$ L_i^- $$ iis the network loss when including all forecast-implied inferences.
- $$ L_i $$ is the network loss with the forecast task.

We then use this score to decide how much the forecast workers should get. The higher their score relative to inference workers, the higher the total reward allocated to forecasters:

$$
\tau_i \equiv \alpha \frac{T_i - \min(0, \max_j T_{ij})}{|\max_j T_{ij}|} + (1 - \alpha) \tau_{i-1}
$$

Where:
- $$ \tau_i $$ is a ratio expressing the relative added value of the forecasting task relative to inference workers.
- $$ T_{ij} $$ is the performance score for each inference worker.

This ratio is then mapped onto a fraction of the worker rewards that is allocated to forecasters:

$$
\chi = \begin{cases}
    0.1 & \text{if } \tau_i < 0, \\
    0.4 \tau_i + 0.1 & \text{if } 0 \leq \tau_i < 1, \\
    0.5 & \text{if } \tau_i \geq 1.
\end{cases}
$$

### The Normalization Factor

We use a normalization factor $$ \gamma $$ to ensure the rewards add up to $$ E_{i,t} $$:

$$
\gamma = \frac{F_i + G_i}{(1 - \chi)F_i + \chi G_i}
$$

Where:
- $$ \gamma $$ ensures that the total reward allocated to workers ($$ U_{i} + V_{i} $$) remains constant after accounting for the added value of the forecasting task.
By using these methods, we can ensure that rewards are spread out fairly and encourage everyone to contribute their best work.
